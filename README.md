# Audio-control-plug-in-for-Tachograph
请各位周五前提交github repository地址，提交前请写好README.md，其中包含：

1）项目内容的功能描述，制作该项目的必要性，与其他现有项目的不同之处。

制作的必要性：现在市面上的行车记录仪app，基本都是没有语音控制功能的，只拥有最简单的行车记录仪功能。而在开车的时候，这是十分不方便的，若是可以通过语音控制行车记录仪app拍照，则可以极大的提高软件使用的便利性，以及行车的安全性。

功能描述：该app可以对用户的声音做出响应，根据对应的声音在用户选定的对应地方触发点击事件，从而实现对一般的行车记录仪app的语音控制拍照，录像的功能。

不同之处：根据调查搜索，通过这种语音触发点击事件来实现对一个普通行车记录仪app的语音控制功能的类似插件的app目前是没有的，预期这个app对一般行车记录仪的实用性会有极大的提高。

2）项目的关键技术，其参考文献。

安卓编程技术、简易的tensorflow应用

语音唤醒模型 https://github.com/tensorflow/models/tree/master/research/audioset

简单词汇识别 https://www.tensorflow.org/tutorials/sequences/audio_recognition

3）完成项目的计划：包括阅读文件，编码，实验，等步骤的完成时间点。

因复习考研，考研于12月底结束，预计一月份开始进行毕业设计，利用假期时间完成，预计使用5-6周：

第一周：阅读相关文献，理解语音唤醒模型的原理和使用
第二周：参考阅读简单词汇识别的源码，装载的andriod环境中
第三、四周：针对项目功能进行编码
第五周：结合实际的行车记录仪app进行实验、改进项目

4）另外请分别用50-100字归纳第1，2次会议的内容，包括文献阅读，毕设项目构思，和导师反馈。

文献阅读

"A Survey on Deep Transfer Learning"

作者的通过调查相关文件，整理出了关于深度迁移学习的调查论文，其具体贡献如下：
① 定义深度迁移学习，并创新性的将其分类为四类：基于实例的深度迁移学习、基于映射的深度迁移学习、基于网络的深度迁移学习、基于对抗的深度迁移学习；
② 回顾了目前关于四类深度迁移学习的研究工作，给出了每个类别的标准化描述和示意图。 

"Contextual Parameter Generation for Universal Neural Machine Translation"

这篇文章是在神经网络机器翻译(NMT)上面进行算法的改进，作者提出了对现有的NMT模型进行简单的修改，加入了语境参数生成器(CPG)。加入CPG之后，能够通过对语言进行特定参数化，使得单个训练出来的通用模型实现在多种语言之间进行转换，达到比普通模型更加优秀的效果。 

第一次会议：我提出了我的第一个项目设想，是与近似商品推荐相关的，但因为实际用途不大，而且思路上也舍近求远，被老师驳回。

第二次会议：针对老师在开会时提到过的选材范例，我提出了做一个给行车记录仪app加上语音控制功能的app的项目思路，得到了通过，并在老师那里得到了相关技术的app范例和参考资料。

